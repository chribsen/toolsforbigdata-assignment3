{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 8 - MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exercises for this week has been executed using regular python scripts and not through IPython. Thus, the results you're seeing in the exercises below is pasted from the stdout of the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Define and implement a MapReduce job to count the occurrences of each word in a text file. Document that it works with a small example._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we are going to count the occurunces of each word in a text file. For this job, we've chosen the shakespeare corpus, since it is a corpus we are familiar with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = RegexpTokenizer(u'\\w+|[^\\w\\s\\ufeff]+')\n",
    "\n",
    "class MRWordCount(MRJob):\n",
    "    def mapper(self, key, line):\n",
    "        for word in r.tokenize(line):\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def combiner(self, word, c):\n",
    "        yield word, sum(c)\n",
    "\n",
    "    def reducer(self, key, vals):\n",
    "        yield key, sum(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how the MapReduce job computes the word occurences:\n",
    "1. The mapper tokenizes each line using `RegexpTokenizer` from nltk. 2. The mapper yields a tuple with the word as a key and 1 as a value.\n",
    "2. The combiner then goes through all the tuples and sums the values for all the keys in all the lines.\n",
    "3. Finally, the reducer sums all the values by the key and values. (This is different from the combiner that did in on a line-level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script is executed using a regular main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    MRWordCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the script we need to provide an input file as an argument. This can be done through the CLI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`python test_mapreduce_word_count.py ../shakespeare.txt > job_output.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the MapReduce job is written to `job_output.txt`. Let's take a look into the file and see if everything looks like what we would expect. To do this, we shuffle the lines by using `gshuf` from `coreutils` (available for OS X through homebrew) and we can take the 10 first lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gshuf text.txt | head -10`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\"disdainful\"    1\n",
    "\"promises\"      1\n",
    "\"wash\"  2\n",
    "\"wound\" 4\n",
    "\"taxing\"        1\n",
    "\"lovers\"        6\n",
    "\"boughs\"        3\n",
    "\"beast\" 3\n",
    "\"goldsmiths\"    1\n",
    "\"content\"       7\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and implement a MapReduce job that determines if a graph has an Euler tour (all vertices have even degree) where you can assume that the graph you get is connected. This file https://www.dropbox.com/s/usdi0wpsqm3jb7f/eulerGraphs.txt?dl=0 has 5 graphs – for each graph, the first line tells the number of nodes N and the number of edges E. The next E lines tells which two nodes are connected by an edge. Two nodes can be connected by multiple edges.\n",
    "\n",
    "It is fine if you split the file into 5 different files. You do not need to keep the node and edge counts in the top of the file.\n",
    "\n",
    "Document that it works using a small example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise will be concerned with finding out whether the graphs provided in the exercise description has an eularian path or has not. This should be a straight-forward task since an eulerian graph is a graph where all the nodes have a even degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following MapReduce job decomposes this task into 3 `MRStep`s. Since this is a coherent class, we won't decompose the code, but we will describe the MapReduce pipeline below:\n",
    "1. The first `MRStep` consists of a mapper that splits the edges into two tuples, each consisting of a node key and a count of 1.\n",
    "The reducer, within the same `MRStep`, then counts the occurunces of each node in the edges (i.e. producing the degree).\n",
    "2. The second `MRStep` then checks if the node has an even degree. If it does, a logic True value is produced.\n",
    "3. the last `MRStep` checks if any node has an odd degree. If it has, there is no eulerian path in the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MRWordCount(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_1,\n",
    "                   reducer=self.reducer_1),\n",
    "            MRStep(reducer=self.reducer_2),\n",
    "            MRStep(reducer=self.reducer_3)\n",
    "        ]\n",
    "\n",
    "    def mapper_1(self, key, line):\n",
    "        e = line.split()\n",
    "        yield e[0], 1\n",
    "        yield e[1], 1\n",
    "\n",
    "    def reducer_1(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "    def reducer_2(self, key, values):\n",
    "        yield 'Euler graph?', values.next() % 2 == 0\n",
    "\n",
    "    def reducer_3(self, key, values):\n",
    "        yield 'Euler graph?', not False in list(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the code is done through the terminal by running the following command for all the graphs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`python test_mapreduce_euler_path.py graphs/graph1.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Graph1 -- \"Euler graph?\"  true\n",
    "Graph2 -- \"Euler graph?\"  false\n",
    "Graph3 -- \"Euler graph?\"  false\n",
    "Graph4 -- \"Euler graph?\"  false\n",
    "Graph5 -- \"Euler graph?\"  false\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only graph that has an eulerian path, thus is the first graph `Graph1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the MapReduce job from the lecture which finds common friends. To test it out, use the example from the slides and this one https://www.dropbox.com/s/ln0maf3q9xa08sf/facebook_combined.txt?dl=0 (note that for the Facebook file, you need to extend the job to convert from a list of edges to the format from the slides – do this with an additional map/reduce job).Document that it works using a small example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a MapReduce job which counts the number of triangles in a graph. Use the following graph http://www.cise.ufl.edu/research/sparse/matrices/SNAP/roadNet-CA.html\n",
    "\n",
    "Document that it works using a small example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 9 - Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we're going to use Apache Spark to make a simple word count. The file that we've choosed for this purpose is a some sample text of the Apache Spark Wikipedia page. The context looks like this:\n",
    "```\n",
    "Spark is a free and open-source software web application framework and domain-specific language written in Java. It is an alternative to other Java web application frameworks such as \n",
    "JAX-RS, Play framework and Spring MVC. It runs on an embedded Jetty web server by default, but can be configured to run on other webservers. Inspired by Sinatra, it does not follow the \n",
    "model–view–controller pattern used in other frameworks, such as Spring MVC. Instead, Spark is intended for \"quickly creating web-applications in Java with minimal effort.\"[1]\n",
    "Spark was created and open-sourced in 2011 by Per Wendel, and was completely rewritten for version 2 in 2014. The rewrite was hugely centered around the Java 8 lambda philosophy, so Java 7 is officially not supported in version 2 and above.\n",
    "```\n",
    "\n",
    "The text is persisted in a file called `sample_file.txt` placed in the working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the file `sample_file.txt` we can use the `Pyspark` function `textFile()` which takes the filename as an argument and returns a RDD object (_Resilient Distributed Dataset_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = sc.textFile('sample_file.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDD Object has currently grouped the file line-by-line. Since we're going to count each word, we need to tokenize each sentence. Since this task is an embarrassingly parallel task (can be partitioned into as many processes as there are lines), we can apply a mapping function. However, since we want to produce multiple key-value pairs (tuples) for each sentence, we need to use `flatMap()`. The `flatMap()` function takes another function, as an argument, that returns a iterable and produces multiple tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize lines\n",
    "rdd = f.flatMap(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tokenized the sentences using a `flatMap` transform, we can make another `map` transform that produces a key-value tuple, which can be reduced by the tokenized word (i.e. the key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map each row\n",
    "rdd = rdd.map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last tranform step, we need to apply a `reduce` transform to the RDD object. Since this task involves multiple tuples, we can't consider it as being an embarrassingly parallel task. This `reduce` step is similar to the `fold` function we're familiar with from functional programming languages, where the variables `a` and `b` represent the current value pair and a variable that is kept in memory during the reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group by count\n",
    "rdd = rdd.reduceByKey(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can call the `collect()` action, which executes the Apache spark job and returns the word count in a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Execute\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[(u'and', 6),\n",
    " (u'Java', 4),\n",
    " (u'JAX-RS,', 1),\n",
    " (u'rewrite', 1),\n",
    " (u'is', 4),\n",
    " (u'supported', 1),\n",
    " (u'Jetty', 1),\n",
    " (u'Per', 1),\n",
    " (u'an', 2),\n",
    " (u'MVC.', 2),\n",
    " (u'as', 2),\n",
    " (u'webservers.', 1),\n",
    " (u'embedded', 1),\n",
    " (u'frameworks,', 1),\n",
    " (u'in', 6),\n",
    " (u'follow', 1),\n",
    " (u'alternative', 1),\n",
    " (u'web-applications', 1),\n",
    " (u'creating', 1),\n",
    " (u'Sinatra,', 1),\n",
    " (u'web', 3),\n",
    " (u'Play', 1),\n",
    " (u'effort.\"[1]', 1),\n",
    " (u'rewritten', 1),\n",
    " (u'for', 2),\n",
    " (u'application', 2),\n",
    " (u'default,', 1),\n",
    " (u'intended', 1),\n",
    " (u'Java.', 1),\n",
    " (u'frameworks', 1),\n",
    " (u'to', 2),\n",
    " (u'written', 1),\n",
    " (u'version', 2),\n",
    " (u'7', 1),\n",
    " (u'model\\u2013view\\u2013controller', 1),\n",
    " (u'8', 1),\n",
    " (u'Spark', 3),\n",
    " (u'officially', 1),\n",
    " (u'configured', 1),\n",
    " (u'minimal', 1),\n",
    " (u'can', 1),\n",
    " (u'be', 1),\n",
    " (u'such', 2),\n",
    " (u'used', 1),\n",
    " (u'run', 1),\n",
    " (u'around', 1),\n",
    " (u'centered', 1),\n",
    " (u'Inspired', 1),\n",
    " (u'open-sourced', 1),\n",
    " (u'free', 1),\n",
    " (u'it', 1),\n",
    " (u'Spring', 2),\n",
    " (u'framework', 2),\n",
    " (u'domain-specific', 1),\n",
    " (u'but', 1),\n",
    " (u'not', 2),\n",
    " (u'philosophy,', 1),\n",
    " (u'The', 1),\n",
    " (u'2014.', 1),\n",
    " (u'by', 3),\n",
    " (u'2011', 1),\n",
    " (u'a', 1),\n",
    " (u'on', 2),\n",
    " (u'runs', 1),\n",
    " (u'open-source', 1),\n",
    " (u'language', 1),\n",
    " (u'created', 1),\n",
    " (u'Wendel,', 1),\n",
    " (u'was', 3),\n",
    " (u'with', 1),\n",
    " (u'It', 2),\n",
    " (u'\"quickly', 1),\n",
    " (u'server', 1),\n",
    " (u'above.', 1),\n",
    " (u'2', 2),\n",
    " (u'so', 1),\n",
    " (u'does', 1),\n",
    " (u'hugely', 1),\n",
    " (u'pattern', 1),\n",
    " (u'completely', 1),\n",
    " (u'the', 2),\n",
    " (u'software', 1),\n",
    " (u'other', 3),\n",
    " (u'Instead,', 1),\n",
    " (u'lambda', 1)]\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we are going to compute the euler tour of the given graphs and see if they contain an eulerian path. Since a eulerian graph cannot contain a node with an odd degree, we can just count the degrees for all the nodes, sum them and see if the x mod 2 is equal to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we need to initialize the data. Prior to the loading, we've partitioned each graph into a different `.txt` document, making it easier to work with. We've also removed the total counts in the start of each graph, since they're not relevant for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = sc.textFile('graphs/graph1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous exercise, we can use a `flatMap()` transform to split the edges into each node. Afterwards, we construct a key-value tuple for counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = f.flatMap(lambda x: x.split())\n",
    "rdd = rdd.map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To count the values by key, we make a `reduce` transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = rdd.reduceByKey(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we do not need the key (the node) anymore, but just the counts. Thus, we transform all those key-value pair into just a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = rdd.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've a list of integers, we can sum them and see if mod 2 equals zero. In case it does, we hav a eulerian path in our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_eularian = int(rdd.sum()) % 2 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The path is eularian'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'The path is eularian' if is_eularian else 'This path is not eularian'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply all this to all the graphs, we can compose everything we just did into a function that takes the filanme as an argument and returns whether the path was eularian or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_graph_eularian(filename):\n",
    "    f = sc.textFile(filename)\n",
    "    rdd = f.flatMap(lambda x: x.split())\n",
    "    rdd = rdd.map(lambda x: (x,1))\n",
    "    rdd = rdd.reduceByKey(lambda a,b: a+b)\n",
    "    rdd = rdd.map(lambda x: x[1] % 2 == 0)\n",
    "    is_eularian = False not in rdd.collect()\n",
    "    return 'The path is eularian' if is_eularian else 'This path is not eularian'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct a `set` of all the filenames and then apply it to all graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = ['graph1.txt', 'graph2.txt', 'graph3.txt', 'graph4.txt', 'graph5.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filename in filenames:\n",
    "    print filename[:-4] + ' -- ' + is_graph_eularian('graphs/'+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "graph1 -- The path is eularian\n",
    "graph2 -- This path is not eularian\n",
    "graph3 -- This path is not eularian\n",
    "graph4 -- This path is not eularian\n",
    "graph5 -- This path is not eularian\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen on the result, the 1st graph has an eularian path, meanwhile all the others do not. This result corresponds to the one we got when doing the same calculation using MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we're going to analyze wifi data. The frist thing we need to do, is to initilize the textfile into Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = sc.textFile('wifi.data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to construct and perform Spark queries on the initialized data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 Most observed networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do, is to get the 10 most observed networks by this particular mobile phone. To do this, we can use the MAC address of the router to uniquely identify each router. But since the MAC address does not make sense by itself, we can combine the `SSID` and `BSSID` in a tuple, and count their occurunces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = f.map(lambda x: eval(x))\n",
    "a = a.map(lambda x: ((x['ssid'], x['bssid']),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above enumerates all (`ssid`,`bssid`) tuples into another tuple, so they are ready to be counted. The code below then reduces the tuples in the `RDD` using `reduceByKey`. This function reduces the tuples by the 0th item in the tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = a.reduceByKey(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the result, but before showing it we would like to sort it by their counts in a descending order. Given a large distributed data set, this is usually a complicated task, but Spark makes it very easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = a.sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to execute our Spark code, by using the `take()` function to get the head 10 tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = a.sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[((u'AirLink5GHz126C18', u'34:21:09:12:6c:1a'), 347),\n",
    " ((u'NETGEAR_1', u'00:24:b2:98:39:d2'), 338),\n",
    " ((u'AirLink126C18', u'34:21:09:12:6c:18'), 324),\n",
    " ((u'Housing People', u'e8:08:8b:c9:c1:79'), 318),\n",
    " ((u'Lausten_5GHz', u'44:94:fc:56:08:fb'), 315),\n",
    " ((u'Bronx', u'00:22:b0:b3:f2:ea'), 314),\n",
    " ((u'Playhouse', u'2c:b0:5d:ef:08:2b'), 272),\n",
    " ((u'Lausten', u'44:94:fc:56:ce:5e'), 240),\n",
    " ((u'Kaspers Wi-Fi-netv\\xe6rk', u'28:cf:e9:84:a1:c3'), 211),\n",
    " ((u'Internet4realz', u'bc:ee:7b:55:1a:43'), 210)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `AirLink5GHz126C18` router is the most observed network by this particular mobile phone within a given time period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 most common wifi names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wifi names often have common names, since people often keep the default SSID on their routers. In this part of the exercise we are going to find out what the most common of these Wifi names are (`SSID`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we need to reinitialize our `rdd` variable `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = f.map(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the data has been reinitialized, we can group the tuples in the `RDD` by `SSID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = a.groupBy(lambda x: x['ssid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost ready to count the wifi names. However, since the `SSID` is repeated for each log entry, we need to remove all duplicates for each router. We can do that by converting the `BSSID` (MAC address of router) into a set and counting the length of the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = a.map(lambda x: (x[0], len(set([v['bssid'] for v in x[1]]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then sort the `RDD` on the count in a descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = a.sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally taking the top 10 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[(u'', 15),\n",
    " (u'SIT-GUEST', 15),\n",
    " (u'SIT-BYOD', 13),\n",
    " (u'SIT-PROD', 13),\n",
    " (u'PDA-105', 11),\n",
    " (u'KNS-105', 11),\n",
    " (u'MED-105', 11),\n",
    " (u'KEA-PUBLIC', 6),\n",
    " (u'GST-105', 5),\n",
    " (u'wireless', 4)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result depicted above reveals that an empty `SSID` is the most common. This might be by routers that is hiding their SSID for security reasons. The router name `SIT-GUEST`, which is equally common as an empty `SSID`, might belong to a larger institute/firm that has multiple router hotspots, with the same names, accross their locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 longest wifi names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need, is to reinitilize the rdd. This time, we will only be in need of the distinct `SSID`s, so we start by removing all unecessary data from the tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we are interested in finding the longest wifi names (i.e. longest `SSID`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = f.map(lambda x: eval(x)['ssid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our tuple only contains the `SSID`, we can remove all the duplicates by calling `distinct()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = a.distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the length of the individual wifi names is a embarrasingly parallel task, so we can use `map()` to count length using `len()` and construct a new tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = a.map(lambda x: (x, len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can sort the wifi names by `SSID` length in a descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = a.sortBy(lambda x: x[1], ascending=False)\n",
    "a.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[(u'HP-Print-43-Deskjet 3520 series', 31),\n",
    " (u'TeliaGatewayA4-B1-E9-2C-9E-CA', 29),\n",
    " (u'TeliaGateway08-76-FF-84-FF-8C', 29),\n",
    " (u'TeliaGateway9C-97-26-57-15-F9', 29),\n",
    " (u'TeliaGateway08-76-FF-46-3E-36', 29),\n",
    " (u'TeliaGateway9C-97-26-57-15-99', 29),\n",
    " (u'TeliaGateway08-76-FF-8A-EE-32', 29),\n",
    " (u'TeliaGateway08-76-FF-85-04-2F', 29),\n",
    " (u'TeliaGateway08-76-FF-9C-E0-82', 29),\n",
    " (u'Charlotte R.s Wi-Fi-netv\\xe6rk', 27)]\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the  result above, we can see that `HP-Print-43-Deskjet 3520 series` is the longest Wifi `SSID` that has been observed by the mobile phone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 10 - Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
